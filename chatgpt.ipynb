{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_python_files(base_paths):\n",
    "    \"\"\"\n",
    "    Extracts the paths, filenames, and content of Python files in specified directories.\n",
    "    \n",
    "    Parameters:\n",
    "        base_paths (list): List of base paths to search for Python files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the file paths as keys and file content as values.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for base_path in base_paths:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.py'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            extracted_data[file_path] = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def save_extracted_data(output_file, extracted_data):\n",
    "    \"\"\"\n",
    "    Saves the extracted data to a file in a readable format.\n",
    "    \n",
    "    Parameters:\n",
    "        output_file (str): Path to the output file.\n",
    "        extracted_data (dict): Dictionary containing file paths and content.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for path, content in extracted_data.items():\n",
    "            f.write(f\"# {path}\\n\")\n",
    "            f.write(f\"{content}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directories to search\n",
    "    directories_to_search = [\"dags\", \"include\"]\n",
    "    # Extract the Python files\n",
    "    python_files = extract_python_files(directories_to_search)\n",
    "    # Save the extracted data to a file\n",
    "    save_extracted_data(\"extracted_python_files.txt\", python_files)\n",
    "    print(\"Extraction completed! Check 'extracted_python_files.txt' for the output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed! Check 'extracted_files.txt' for the output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_files(base_paths, extensions):\n",
    "    \"\"\"\n",
    "    Extracts the paths, filenames, and content of files with specified extensions in given directories.\n",
    "    \n",
    "    Parameters:\n",
    "        base_paths (list): List of base paths to search for files.\n",
    "        extensions (list): List of file extensions to include (e.g., ['.py', '.yml']).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the file paths as keys and file content as values.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for base_path in base_paths:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in extensions):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            extracted_data[file_path] = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def save_extracted_data(output_file, extracted_data):\n",
    "    \"\"\"\n",
    "    Saves the extracted data to a file in a readable format.\n",
    "    \n",
    "    Parameters:\n",
    "        output_file (str): Path to the output file.\n",
    "        extracted_data (dict): Dictionary containing file paths and content.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for path, content in extracted_data.items():\n",
    "            f.write(f\"# {path}\\n\")\n",
    "            f.write(f\"{content}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directories to search\n",
    "    directories_to_search = [\"dags\", \"include\"]\n",
    "    # Specify the file extensions to include\n",
    "    file_extensions = [\".py\", \".yml\"]\n",
    "    # Extract the files\n",
    "    extracted_files = extract_files(directories_to_search, file_extensions)\n",
    "    # Save the extracted data to a file\n",
    "    save_extracted_data(\"extracted_files.txt\", extracted_files)\n",
    "    print(\"Extraction completed! Check 'extracted_files.txt' for the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Inicializando a API do Kaggle\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "def get_kaggle_metadata(dataset_id):\n",
    "    \"\"\"\n",
    "    Função para obter os metadados de um dataset Kaggle e exibir os campos disponíveis.\n",
    "    \n",
    "    Parâmetros:\n",
    "        dataset_id (str): ID do dataset no formato 'username/dataset-name'.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Metadados do dataset.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Baixar metadados do dataset\n",
    "        api.dataset_metadata(dataset_id, path=temp_dir)\n",
    "        \n",
    "        # Caminho do arquivo de metadados\n",
    "        metadata_path = os.path.join(temp_dir, \"dataset-metadata.json\")\n",
    "        \n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"Metadados para '{dataset_id}' não encontrados.\")\n",
    "            return None\n",
    "        \n",
    "        # Carregar os metadados\n",
    "        with open(metadata_path, \"r\") as file:\n",
    "            metadata = json.load(file)\n",
    "        \n",
    "        print(f\"Metadados disponíveis para o dataset '{dataset_id}':\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "# ID do dataset Kaggle\n",
    "dataset_id = \"olistbr/marketing-funnel-olist\"  # Altere para o dataset que deseja verificar\n",
    "\n",
    "# Chamando a função e exibindo os metadados\n",
    "metadata = get_kaggle_metadata(dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
