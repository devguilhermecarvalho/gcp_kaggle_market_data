{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed! Check 'extracted_python_files.txt' for the output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_python_files(base_paths):\n",
    "    \"\"\"\n",
    "    Extracts the paths, filenames, and content of Python files in specified directories.\n",
    "    \n",
    "    Parameters:\n",
    "        base_paths (list): List of base paths to search for Python files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the file paths as keys and file content as values.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for base_path in base_paths:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.py'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            extracted_data[file_path] = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def save_extracted_data(output_file, extracted_data):\n",
    "    \"\"\"\n",
    "    Saves the extracted data to a file in a readable format.\n",
    "    \n",
    "    Parameters:\n",
    "        output_file (str): Path to the output file.\n",
    "        extracted_data (dict): Dictionary containing file paths and content.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for path, content in extracted_data.items():\n",
    "            f.write(f\"# {path}\\n\")\n",
    "            f.write(f\"{content}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directories to search\n",
    "    directories_to_search = [\"dags\", \"include\"]\n",
    "    # Extract the Python files\n",
    "    python_files = extract_python_files(directories_to_search)\n",
    "    # Save the extracted data to a file\n",
    "    save_extracted_data(\"extracted_python_files.txt\", python_files)\n",
    "    print(\"Extraction completed! Check 'extracted_python_files.txt' for the output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed! Check 'extracted_files.txt' for the output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def extract_files(base_paths, extensions):\n",
    "    \"\"\"\n",
    "    Extracts the paths, filenames, and content of files with specified extensions in given directories.\n",
    "    \n",
    "    Parameters:\n",
    "        base_paths (list): List of base paths to search for files.\n",
    "        extensions (list): List of file extensions to include (e.g., ['.py', '.yml']).\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with the file paths as keys and file content as values.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for base_path in base_paths:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if any(file.endswith(ext) for ext in extensions):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            extracted_data[file_path] = f.read()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def save_extracted_data(output_file, extracted_data):\n",
    "    \"\"\"\n",
    "    Saves the extracted data to a file in a readable format.\n",
    "    \n",
    "    Parameters:\n",
    "        output_file (str): Path to the output file.\n",
    "        extracted_data (dict): Dictionary containing file paths and content.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for path, content in extracted_data.items():\n",
    "            f.write(f\"# {path}\\n\")\n",
    "            f.write(f\"{content}\\n\")\n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the directories to search\n",
    "    directories_to_search = [\"dags\", \"include\"]\n",
    "    # Specify the file extensions to include\n",
    "    file_extensions = [\".py\", \".yml\"]\n",
    "    # Extract the files\n",
    "    extracted_files = extract_files(directories_to_search, file_extensions)\n",
    "    # Save the extracted data to a file\n",
    "    save_extracted_data(\"extracted_files.txt\", extracted_files)\n",
    "    print(\"Extraction completed! Check 'extracted_files.txt' for the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadados disponíveis para o dataset 'olistbr/marketing-funnel-olist':\n",
      "- id: /marketing-funnel-olist\n",
      "- id_no: 78342\n",
      "- datasetSlugNullable: marketing-funnel-olist\n",
      "- ownerUserNullable: None\n",
      "- usabilityRatingNullable: 1.0\n",
      "- titleNullable: Marketing Funnel by Olist\n",
      "- subtitleNullable: 8k leads, closed deals and connection to 100k orders\n",
      "- descriptionNullable: # Marketing Funnel by Olist\n",
      "\n",
      "Welcome! This is a marketing funnel dataset from sellers that filled-in requests of contact to sell their products on [Olist Store](http://www.olist.com). The dataset has information of 8k Marketing Qualified Leads (MQLs) that requested contact between Jun. 1st 2017 and Jun 1st 2018. They were randomly sampled from the total of MQLs. \n",
      "\n",
      "Its features allows viewing a sales process from multiple dimensions: lead category, catalog size, behaviour profile, etc. \n",
      "\n",
      "This is real data, it has been anonymized and sampled from the original dataset.\n",
      "\n",
      "## Joining with Brazilian E-Commerce Public Dataset by Olist\n",
      "This dataset can also be linked to the [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/olistbr/brazilian-ecommerce/home) using ```seller_id```. There you will find information of 100k orders, price, payment, freight performance, customer location, product attributes and finally reviews written by customers.\n",
      "\n",
      "**Instructions on joining are available on this [Kernel](https://www.kaggle.com/andresionek/joining-marketing-funnel-with-brazilian-e-commerce).**\n",
      "\n",
      "## Context\n",
      "This dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on our website: [www.olist.com](https://www.olist.com)\n",
      "\n",
      "**A seller join Olist through a marketing and sales funnel that was made public at this dataset. Description of steps:**\n",
      "\n",
      "1. Sign-up at a landing page.\n",
      "1. Get contacted by a Sales development Representative (SDR), confirm some information and schedule a consultancy.\n",
      "1. Consultancy is made by a Sales Representative (SR). The SR may close the deal (lead sing up) or lose the deal (led leaves without sign in) \n",
      "1. Lead becomes a seller and starts building his catalog on Olist. \n",
      "1. His products are published on marketplaces and ready to sell!\n",
      "\n",
      "### Attention\n",
      "1. A seller MQL might come from multiple sources (he might subscribe on two different landing pages, for instance).\n",
      "\n",
      "### Examples of  Landing Pages\n",
      "![Example of a landing page](https://i.imgur.com/jKZTP5e.png)\n",
      "![](https://i.imgur.com/mAljYcq.png)\n",
      "\n",
      "## Data Schema\n",
      "The data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:\n",
      "![](https://i.imgur.com/Jory0O3.png)\n",
      "\n",
      "## Inspiration\n",
      "Here are some inspiration for possible outcomes from this dataset.\n",
      "\n",
      "**Customer Lifetime Value:** <br>\n",
      "How much a customer will bring in future revenue?\n",
      "\n",
      "**SR/SDR Optimization:**<br> \n",
      "Which SR or SDR should talk with each kind of lead?\n",
      "\n",
      "**Closing Prediction:**<br> \n",
      "Which deals will be closed?\n",
      "\n",
      "**EDA:**<br> \n",
      "Just Have Fun!\n",
      "\n",
      "## Acknowledgements\n",
      "Thanks to Olist for releasing this dataset.\n",
      "- datasetId: 78342\n",
      "- datasetSlug: marketing-funnel-olist\n",
      "- hasDatasetSlug: True\n",
      "- ownerUser: \n",
      "- hasOwnerUser: False\n",
      "- usabilityRating: 1.0\n",
      "- hasUsabilityRating: True\n",
      "- totalViews: 144277\n",
      "- totalVotes: 283\n",
      "- totalDownloads: 14617\n",
      "- title: Marketing Funnel by Olist\n",
      "- hasTitle: True\n",
      "- subtitle: 8k leads, closed deals and connection to 100k orders\n",
      "- hasSubtitle: True\n",
      "- description: # Marketing Funnel by Olist\n",
      "\n",
      "Welcome! This is a marketing funnel dataset from sellers that filled-in requests of contact to sell their products on [Olist Store](http://www.olist.com). The dataset has information of 8k Marketing Qualified Leads (MQLs) that requested contact between Jun. 1st 2017 and Jun 1st 2018. They were randomly sampled from the total of MQLs. \n",
      "\n",
      "Its features allows viewing a sales process from multiple dimensions: lead category, catalog size, behaviour profile, etc. \n",
      "\n",
      "This is real data, it has been anonymized and sampled from the original dataset.\n",
      "\n",
      "## Joining with Brazilian E-Commerce Public Dataset by Olist\n",
      "This dataset can also be linked to the [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/olistbr/brazilian-ecommerce/home) using ```seller_id```. There you will find information of 100k orders, price, payment, freight performance, customer location, product attributes and finally reviews written by customers.\n",
      "\n",
      "**Instructions on joining are available on this [Kernel](https://www.kaggle.com/andresionek/joining-marketing-funnel-with-brazilian-e-commerce).**\n",
      "\n",
      "## Context\n",
      "This dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on our website: [www.olist.com](https://www.olist.com)\n",
      "\n",
      "**A seller join Olist through a marketing and sales funnel that was made public at this dataset. Description of steps:**\n",
      "\n",
      "1. Sign-up at a landing page.\n",
      "1. Get contacted by a Sales development Representative (SDR), confirm some information and schedule a consultancy.\n",
      "1. Consultancy is made by a Sales Representative (SR). The SR may close the deal (lead sing up) or lose the deal (led leaves without sign in) \n",
      "1. Lead becomes a seller and starts building his catalog on Olist. \n",
      "1. His products are published on marketplaces and ready to sell!\n",
      "\n",
      "### Attention\n",
      "1. A seller MQL might come from multiple sources (he might subscribe on two different landing pages, for instance).\n",
      "\n",
      "### Examples of  Landing Pages\n",
      "![Example of a landing page](https://i.imgur.com/jKZTP5e.png)\n",
      "![](https://i.imgur.com/mAljYcq.png)\n",
      "\n",
      "## Data Schema\n",
      "The data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:\n",
      "![](https://i.imgur.com/Jory0O3.png)\n",
      "\n",
      "## Inspiration\n",
      "Here are some inspiration for possible outcomes from this dataset.\n",
      "\n",
      "**Customer Lifetime Value:** <br>\n",
      "How much a customer will bring in future revenue?\n",
      "\n",
      "**SR/SDR Optimization:**<br> \n",
      "Which SR or SDR should talk with each kind of lead?\n",
      "\n",
      "**Closing Prediction:**<br> \n",
      "Which deals will be closed?\n",
      "\n",
      "**EDA:**<br> \n",
      "Just Have Fun!\n",
      "\n",
      "## Acknowledgements\n",
      "Thanks to Olist for releasing this dataset.\n",
      "- hasDescription: True\n",
      "- isPrivate: False\n",
      "- keywords: ['business', 'finance', 'marketing', 'internet', 'data analytics']\n",
      "- licenses: [{'nameNullable': 'CC-BY-NC-SA-4.0', 'name': 'CC-BY-NC-SA-4.0', 'hasName': True}]\n",
      "- collaborators: [{'usernameNullable': 'andresionek', 'groupSlugNullable': None, 'username': 'andresionek', 'hasUsername': True, 'groupSlug': '', 'hasGroupSlug': False, 'role': 'writer', 'collaboratorTypeCase': 'username'}, {'usernameNullable': 'dabague', 'groupSlugNullable': None, 'username': 'dabague', 'hasUsername': True, 'groupSlug': '', 'hasGroupSlug': False, 'role': 'writer', 'collaboratorTypeCase': 'username'}, {'usernameNullable': 'andresionek', 'groupSlugNullable': None, 'username': 'andresionek', 'hasUsername': True, 'groupSlug': '', 'hasGroupSlug': False, 'role': 'admin', 'collaboratorTypeCase': 'username'}]\n",
      "- data: []\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Inicializando a API do Kaggle\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "def get_kaggle_metadata(dataset_id):\n",
    "    \"\"\"\n",
    "    Função para obter os metadados de um dataset Kaggle e exibir os campos disponíveis.\n",
    "    \n",
    "    Parâmetros:\n",
    "        dataset_id (str): ID do dataset no formato 'username/dataset-name'.\n",
    "    \n",
    "    Retorna:\n",
    "        dict: Metadados do dataset.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Baixar metadados do dataset\n",
    "        api.dataset_metadata(dataset_id, path=temp_dir)\n",
    "        \n",
    "        # Caminho do arquivo de metadados\n",
    "        metadata_path = os.path.join(temp_dir, \"dataset-metadata.json\")\n",
    "        \n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"Metadados para '{dataset_id}' não encontrados.\")\n",
    "            return None\n",
    "        \n",
    "        # Carregar os metadados\n",
    "        with open(metadata_path, \"r\") as file:\n",
    "            metadata = json.load(file)\n",
    "        \n",
    "        print(f\"Metadados disponíveis para o dataset '{dataset_id}':\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"- {key}: {value}\")\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "# ID do dataset Kaggle\n",
    "dataset_id = \"olistbr/marketing-funnel-olist\"  # Altere para o dataset que deseja verificar\n",
    "\n",
    "# Chamando a função e exibindo os metadados\n",
    "metadata = get_kaggle_metadata(dataset_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
